# CNN-Attention-Collection
collect all attention module for cnn networks

1CVPR2023-biformer，即插即用，基于动态稀疏注意力构建高效金字塔网络架构。实测，在多个数据集上能涨点​
Title: BiFormer: Vision Transformer with Bi-Level Routing Attention​
Paper: https://arxiv.org/pdf/2303.08810.pdf​
Code:  https://github.com/rayleizhu/BiFormer​
​

2 CVPR 2022 清华开源ACmix：自注意力和CNN的融合！性能速度全面提升​
ACMix：On the Integration of Self-Attention and Convolution​
https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.pdf​
https://github.com/LeapLabTHU/ACmix​


3 ICLR 2023 面向移动端的新方法，SeaFormer中轻量高效即插即用注意力模块sea_AttentionBlock，在多个数据集上能涨点​
Title: SEAFORMER: SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE SEMANTIC SEGMENTATION​
Paper: https://openreview.net/pdf?id=-qg8MQNrxZw​
Github: https://anonymous.4open.science/r/SeaFormer_anonymous-5C8C​
​

4 ICASSP2023-全新注意力机制EMA，基于跨空间学习的高效多尺度注意力​
Efficient Multi-Scale Attention Module with Cross-Spatial Learning​
https://arxiv.org/vc/arxiv/papers/2305/2305.13563v1.pdf​
https://github.com/yoloonme/ema-attention-module​


5 ICCV 2023 -LSKNet，适合遥感旋转目标检测，利用网络中的LSKblockAttention，助力小目标检测​
Large Selective Kernel Network for Remote Sensing Object Detection​
Paper：https://arxiv.org/pdf/2303.09030.pdf​
代码：https://github.com/zcablii/Large-Selective-Kernel-Networ​
​

6.2023-RevCol，最新可变形大核注意力，超越自注意力D-LKA Attention，暴力涨点​
Reversible Column Networks​
论文链接：https://arxiv.org/pdf/2212.11696.pdf​
代码链接：https://github.com/megvii-research/RevCol​
​

7. arxiv2021引入极化自注意力（Polarized Self-Attention）​
论文链接：https://arxiv.org/pdf/2107.00782.pdf​
官网代码：https://github.com/DeLightCMU/PSA （暂未开源）​
核心代码：https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/attention/PolarizedSelfAttention.py​
​

8. NIPS 2018 引入Double Attention Networks​
https://proceedings.neurips.cc/paper_files/paper/2018/file/e165421110ba03099a1c0393373c5b43-Paper.pdf​
https://github.com/nguyenvo09/Double-Attention-Network​
​

9. CVPR2019，引入即插即用的SKAttention​
https://arxiv.org/pdf/1903.06586.pdf​
https://github.com/implus/SKNet​
​

10. arxiv2021 引入用于微小目标检测的上下文增强和特征细化网络ContextAggregation​
https://arxiv.org/pdf/2106.01401.pdf​
https://github.com/allenai/container​
​

11. arxiv2021 引入MobileViTAttention，助力小目标涨点。移动端轻量通用视觉transformer​
https://arxiv.org/pdf/2110.02178v2.pdf​
https://github.com/apple/ml-cvnets​
​

12. CVPR1018 引入non-local注意力，助力小目标检测​
https://arxiv.org/pdf/1711.07971v3.pdf​
https://github.com/facebookresearch/video-nonlocal-net​
​

13. CVPR2020 | SEAM：弱监督语义分割的自监督注意力机制，小目标遮挡物性能提升​
论文地址：arxiv.org/pdf/2004.04581.pdf​
代码地址：github.com/YudeWang/SEAM​
​

14. arxiv2023引入即插即用CloFormer，注意力机制与卷积的完美融合​
Title: Rethinking Local Perception in Lightweight Vision Transformer​
Paper: https://arxiv.org/pdf/2303.17803.pdf​
https://github.com/qhfan/CloFormer​
​

15. 引入CBAM注意力机制​
https://arxiv.org/pdf/1807.06521v2.pdf​
https://github.com/xmu-xiaoma666/External-Attention-pytorch​
​

16. 引入ECA注意力机制​
https://arxiv.org/pdf/1910.03151.pdf​
https://github.com/BangguWu/ECANet​


17.引入Efficient se注意力机制​
论文地址：https://arxiv.org/pdf/1911.06667.pdf​
项目地址：https://github.com/youngwanLEE/CenterMask​
​

18. 引入GAM注意力机制.​
https://arxiv.org/pdf/2112.05561v1.pdf​
https://github.com/northBeggar/Plug-and-Play/blob/main/GAM%20attention.py​
​

19. 引入Global Context注意力机制​
https://arxiv.org/pdf/2012.13375v1.pdf​
https://github.com/xvjiarui/GCNet​
​

20. 引入Ge注意力机制​
https://arxiv.org/pdf/1810.12348v3.pdf​
https://github.com/hujie-frank/GENet​
​

21. ICCV2023-引入LSKnet中Lskblock注意力块​
Paper：https://arxiv.org/pdf/2303.09030.pdf​
代码：https://github.com/zcablii/Large-Selective-Kernel-Network​
​

22. 引入Parnet attention注意力机制​
论文地址：https://arxiv.org/pdf/2110.07641.pdf​
仓库地址：https://github.com/imankgoyal/NonDeepNetworks​
​

23. 引入SE注意力机制​
https://arxiv.org/pdf/1709.01507.pdf​
https://github.com/hujie-frank/SENet​
​

24. 引入SGE注意力机制​
https://arxiv.org/pdf/1905.09646v2.pdf​
https://github.com/implus/PytorchInsight

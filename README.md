# CNN-Attention-Collection
collect all attention module for cnn networks

1CVPR2023-biformer，即插即用，基于动态稀疏注意力构建高效金字塔网络架构。实测，在多个数据集上能涨点​
Title: BiFormer: Vision Transformer with Bi-Level Routing Attention​
Paper: https://arxiv.org/pdf/2303.08810.pdf​
Code:  https://github.com/rayleizhu/BiFormer​
​
2 CVPR 2022 清华开源ACmix：自注意力和CNN的融合！性能速度全面提升​
ACMix：On the Integration of Self-Attention and Convolution​
https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.pdf​
https://github.com/LeapLabTHU/ACmix​
3 ICLR 2023 面向移动端的新方法，SeaFormer中轻量高效即插即用注意力模块sea_AttentionBlock，在多个数据集上能涨点​
Title: SEAFORMER: SQUEEZE-ENHANCED AXIAL TRANSFORMER FOR MOBILE SEMANTIC SEGMENTATION​
Paper: https://openreview.net/pdf?id=-qg8MQNrxZw​
Github: https://anonymous.4open.science/r/SeaFormer_anonymous-5C8C​
​
4 ICASSP2023-全新注意力机制EMA，基于跨空间学习的高效多尺度注意力​
Efficient Multi-Scale Attention Module with Cross-Spatial Learning​
https://arxiv.org/vc/arxiv/papers/2305/2305.13563v1.pdf​
https://github.com/yoloonme/ema-attention-module​
​
17.引入Efficient se注意力机制​
论文地址：https://arxiv.org/pdf/1911.06667.pdf​
项目地址：https://github.com/youngwanLEE/CenterMask​
​
18. 引入GAM注意力机制.​
https://arxiv.org/pdf/2112.05561v1.pdf​
https://github.com/northBeggar/Plug-and-Play/blob/main/GAM%20attention.py​
​
19. 引入Global Context注意力机制​
https://arxiv.org/pdf/2012.13375v1.pdf​
https://github.com/xvjiarui/GCNet​
​
20. 引入Ge注意力机制​
https://arxiv.org/pdf/1810.12348v3.pdf​
https://github.com/hujie-frank/GENet​
​
21. ICCV2023-引入LSKnet中Lskblock注意力块​
Paper：https://arxiv.org/pdf/2303.09030.pdf​
代码：https://github.com/zcablii/Large-Selective-Kernel-Network​
​
22. 引入Parnet attention注意力机制​
论文地址：https://arxiv.org/pdf/2110.07641.pdf​
仓库地址：https://github.com/imankgoyal/NonDeepNetworks​
​
23. 引入SE注意力机制​
https://arxiv.org/pdf/1709.01507.pdf​
https://github.com/hujie-frank/SENet​
​
24. 引入SGE注意力机制​
https://arxiv.org/pdf/1905.09646v2.pdf​
https://github.com/implus/PytorchInsight
